{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4be03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dustgoggles.func import zero\n",
    "from killscreen.utilities import roundstring\n",
    "\n",
    "# hacky; can remove if we decide to add an install script or put this in the repo root\n",
    "os.chdir(globals()['_dh'][0].parent)\n",
    "\n",
    "from subset.benchmark.bench_utils import (\n",
    "    check_existing_benchmarks,\n",
    "    dump_bandwidth_allowance_metrics,\n",
    "    dump_throwaway_results\n",
    ")\n",
    "from subset.benchmark.handlers import (\n",
    "    execute_test_case, interpret_benchmark_instructions, process_bench_stats\n",
    ")\n",
    "from subset.utilz.generic import summarize_stat\n",
    "from subset.utilz.mount_s3 import mount_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51da70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "simple execution environment for dataset benchmarking. executes benchmarks defined in the benchmark_settings module.\n",
    "\n",
    "important notes:\n",
    "* the `fsspec`-based \"s3\" and \"s3_section\" loaders will only function on the barentsen/cloud-support astropy branch.\n",
    "* other loaders will only function if `goofys` is executable from the user's path.\n",
    "* attempts to access private buckets, via either `goofys` or `fsspec`, will use credentials stored in ~/.aws/credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf375d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    # max files: tests will use the top n_files elements of the TEST_FILES\n",
    "    # attribute of each benchmark module (unless less than n_files are given\n",
    "    # in TEST_FILES)\n",
    "    \"n_files\": 25,\n",
    "    # do we actually want to look at the cuts? probably not in big bulk\n",
    "    # benchmarks, but can be useful for diagnostics or if you want a\n",
    "    # screensaver or whatever.\n",
    "    \"return_cuts\": False,\n",
    "    # rng seed for consistent execution w/out having to explicitly define\n",
    "    # a very long list of rectangles\n",
    "    \"seed\": 123456,\n",
    "    # note that bandwidth throttling is not available in the TIKE environment.\n",
    "}\n",
    "\n",
    "# these correspond to the names of submodules of the benchmark_settings module\n",
    "BENCHMARK_NAMES = (\n",
    "    \"hst\", \"panstarrs\", \"galex_rice\", \"jwst_crf\", \"tesscut\", \"galex_gzip\",\n",
    "    \"hst_big\", \"spitzer_irac\", \"spitzer_cosmos_irac\"\n",
    ")\n",
    "# if there is an existing result file corresponding to a particular\n",
    "# test case, shall we run it again (w/incrementing suffixes attached to\n",
    "# outputs?)\n",
    "DUPLICATE_BENCHMARKS = True\n",
    "# where shall we write benchmark results?\n",
    "METRIC_DIRECTORY = \"subset/benchmark/bench_results\"\n",
    "Path(METRIC_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "# running throwaway tests using a FUSE loader completely cheeses\n",
    "# the statistics without remount. TODO item to run the throwaways\n",
    "# with an fsspec-backed loader here.\n",
    "N_THROWAWAYS = 0\n",
    "\n",
    "# \"benchmarks\" contains instructions for each test case\n",
    "benchmarks = {\n",
    "    name: interpret_benchmark_instructions(name, SETTINGS)\n",
    "    for name in BENCHMARK_NAMES\n",
    "}\n",
    "# TIKE has pre-established per-bucket goofys mountpoints, which we squeeze\n",
    "# into the benchmark definitions\n",
    "mountpoints = {'stpubdata': \"/s3/gf/stpubdata\", 'nishapur': '/s3/gf/nishapur'}\n",
    "for name, benchmark in benchmarks.items():\n",
    "    for case in benchmark:\n",
    "        if case['bucket'] is not None:\n",
    "            case['paths'] = [str(Path(mountpoints[case['bucket']], p)) for p in case['paths']]\n",
    "            \n",
    "# we cannot dynamically remount buckets on TIKE, so we pass a dummy \n",
    "# mount function to the handler.\n",
    "remount = zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df643ce-0198-4471-852f-3db6de12060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter repetitive benchmarks\n",
    "for name, benchmark in benchmarks.items():\n",
    "    deletions = []\n",
    "    for ix, case in enumerate(benchmark):\n",
    "        if (\n",
    "            'astropy' not in case['title']\n",
    "            or 'greedy' in case['title']\n",
    "            or 'preload' in case['title']\n",
    "            or case['count'] != 5\n",
    "            or case['shape'] != (40, 40)\n",
    "            or 'panstarrs' not in case['title']\n",
    "        ):\n",
    "            deletions.append(ix)\n",
    "    benchmarks[name] = [\n",
    "        benchmark[ix] for ix in range(len(benchmark)) if ix not in deletions\n",
    "    ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafeff5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "execute all benchmarks in a loop and save results. you can mess with this cell\n",
    "to pretty straightforwardly look at only certain categories of cases,\n",
    "suppress results, etc.\n",
    "\"\"\"\n",
    "logs = {}\n",
    "for benchmark_name, benchmark in benchmarks.items():\n",
    "    previous = {}\n",
    "    for case in benchmark:\n",
    "        skip, suffix = check_existing_benchmarks(\n",
    "            case['title'], DUPLICATE_BENCHMARKS, METRIC_DIRECTORY\n",
    "        )\n",
    "        if skip is True:\n",
    "            continue\n",
    "        cuts, stat, log, throwaways = execute_test_case(\n",
    "            case, previous, remount, N_THROWAWAYS\n",
    "        )\n",
    "        print(roundstring(summarize_stat(stat)) + \"\\n\")\n",
    "        logs[f\"{case['title']}\"] = log\n",
    "        process_bench_stats(log, case, benchmark_name).to_csv(\n",
    "            Path(METRIC_DIRECTORY, f\"{case['title']}_benchmark_{suffix}.csv\"),\n",
    "            index=None\n",
    "        )\n",
    "        # note that we can't dump bandwidth allowance metrics on TIKE\n",
    "        dump_throwaway_results(throwaways, case['title'], suffix, METRIC_DIRECTORY)\n",
    "        previous = case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
