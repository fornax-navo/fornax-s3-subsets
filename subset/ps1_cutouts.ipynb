{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "from cytoolz.curried import compose, curry, get, get_in, groupby\n",
    "from killscreen.monitors import Netstat, Stopwatch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyarrow import parquet\n",
    "\n",
    "# hacky; can remove if we decide to add an install script or put this in the repo root\n",
    "os.chdir(globals()['_dh'][0].parent)\n",
    "\n",
    "from subset.science.handlers import (\n",
    "    filter_ps1_catalog, sample_ps1_catalog, get_corresponding_images, bulk_skycut\n",
    ")\n",
    "from subset.science.ps1_utils import (\n",
    "    ps1_stack_path, request_ps1_cutout, PS1_CUT_CONSTANTS\n",
    ")\n",
    "from subset.science.science_utils import normalize_range, centile_clip\n",
    "from subset.utilz.generic import make_loaders, parse_topline\n",
    "from subset.utilz.mount_s3 import mount_bucket\n",
    "\n",
    "# suppress irrelevant warnings from matplotlib\n",
    "warnings.filterwarnings(\"ignore\", message=\"More than 20 figures\")\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# what bucket are our images and metadata files stored in?\n",
    "BUCKET = 'nishapur'\n",
    "# where, on the local filesystem, shall we create a FUSE mount for that bucket?\n",
    "S3_ROOT = '/home/ubuntu/s3'\n",
    "if not os.path.exists(S3_ROOT):\n",
    "    os.mkdir(S3_ROOT)\n",
    "# mount that bucket to read metadata\n",
    "mount_bucket(remount=False, mount_path=S3_ROOT, bucket=BUCKET)\n",
    "# catalog of all mean objects from 1000 PS1 sky cells randomly selected from\n",
    "# \"extragalactic\" cells that overlap the viewports of GALEX visits, then filtered\n",
    "# to the \"best\" objects (qualityFlag bit 0b100000) with valid photometry in both\n",
    "# g and z bands (this filter leaves roughly 3% of total sources). other\n",
    "# similarly-formatted catalog files can be used.\n",
    "CATALOG_FN = \"ps1_eg_eclipses_subset_best_gz_coregistered.parquet\"\n",
    "if not Path(CATALOG_FN).exists():\n",
    "    shutil.copy(Path(S3_ROOT, \"ps1/metadata\", CATALOG_FN), Path(CATALOG_FN))\n",
    "catalog = parquet.read_table(CATALOG_FN).to_pandas()\n",
    "# cutouts in dimensions: ra, dec in degrees. treated as side lengths of a rectangle.\n",
    "CUT_SHAPE = (60 / 3600, 60 / 3600)\n",
    "# restrict to sources bright in both g and z? set to 'None' for no cutoff.\n",
    "MAG_CUTOFF = 20\n",
    "# restrict to only sources flagged as extended / not extended?\n",
    "# \"extended\", \"point\", or None for no restriction\n",
    "EXTENSION_TYPE = \"extended\"\n",
    "# restrict to only sources with a valid stack detection? (probably a good idea)\n",
    "STACK_ONLY = True\n",
    "# which PS1 bands are we considering? (only g and z are currently staged, but you can stage more.)\n",
    "PS1_BANDS = (\"g\", \"z\")\n",
    "# how many targets shall we randomly select?\n",
    "TARGET_COUNT = 30\n",
    "# optional parameter -- restrict the total number of PS1 source cells to test the\n",
    "# performance effects of denser sampling. 1000 total cells are available in this test set.\n",
    "# note that the total number of images accessed is number of cells * number of bands.\n",
    "MAX_CELL_COUNT = 8\n",
    "# select loaders -- options are \"astropy\", \"fitsio\", \"greedy_astropy\", \"greedy_fitsio\"\n",
    "# NOTE: because all the files this particular notebook is looking\n",
    "# at are RICE-compressed, there is unlikely to be much difference\n",
    "# between astropy and greedy_astropy -- astropy does not support\n",
    "# loading individual tiles from a a tile-compressed FITS file.\n",
    "LOADERS = make_loaders(\"fitsio\",)\n",
    "# per-loader performance-tuning parameters. you don't need to mess with these\n",
    "# if you don't care about fiddly performance stuff.\n",
    "# chunksize: how many images shall we initialize at once?\n",
    "# threads['image']: how many threads shall we init with in parallel? (None to disable.)\n",
    "# threads['cut']: how many threads shall we cut with in parallel? (None to disable.)\n",
    "# note that S3 handles parallel requests very well; on a smaller instance, you will\n",
    "# run out of CPU or bandwidth before you exhaust its willingness to serve parallel requests.\n",
    "TUNING = {\n",
    "    \"fitsio\": {\n",
    "        \"chunksize\": 200, \"threads\": {\"image\": max(cpu_count() * 4, 20), \"cut\": max(cpu_count() * 4, 20)}\n",
    "    },\n",
    "    \"greedy_fitsio\": {\"chunksize\": 10, \"threads\": {\"image\": cpu_count() * 2, \"cut\": None}},\n",
    "    \"default\": {\"chunksize\": 20, \"threads\": {\"image\": cpu_count() * 4, \"cut\": cpu_count() * 4}},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## target selection\n",
    "the next cell picks a random sample of targets that satisfy the parameters defined above.\n",
    "you can run it again to 'reroll' and pick a new set of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all sources that fit characteristic criteria\n",
    "candidate_sources = filter_ps1_catalog(catalog, MAG_CUTOFF, EXTENSION_TYPE, STACK_ONLY)\n",
    "# randomly-selected subset of those sources w/adequate metadata for cutout definition\n",
    "targets = sample_ps1_catalog(candidate_sources, TARGET_COUNT, MAX_CELL_COUNT)\n",
    "# add requested cut shape instructions to these target definitions\n",
    "targets = [t | {'ra_x': CUT_SHAPE[0], 'dec_x': CUT_SHAPE[1]} for t in targets]\n",
    "# make lists of the ps1 stack images these sources lie within\n",
    "# (so that we can easily initialize each relevant image only once)\n",
    "ps1_stacks, _ = get_corresponding_images(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## cutout retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logs = {}\n",
    "for loader_name, loader in LOADERS.items():\n",
    "    # remount bucket to avoid \"cheating\" -- note that this is still a little cheaty\n",
    "    # because of unreliable, unmodifiable, and entirely black-box caching on S3 side: if you run\n",
    "    # multiple loaders, loaders later in the list will tend to do better. for a 'fairer' \n",
    "    # comparison, reroll between each loader or juice serverside caching with throwaway tests \n",
    "    # (see benchmarking suite)\n",
    "    print(f\"----testing {loader_name}----\")\n",
    "    mount_bucket(remount=True, mount_path=S3_ROOT, bucket=BUCKET)\n",
    "    tuning_params = TUNING[loader_name] if loader_name in TUNING.keys() else TUNING[\"default\"]\n",
    "    cuts, logs[loader_name] = bulk_skycut(\n",
    "        ps1_stacks,\n",
    "        targets,\n",
    "        loader=loader,\n",
    "        return_cuts=True,\n",
    "        data_root=f\"{S3_ROOT}/ps1\",\n",
    "        bands=PS1_BANDS,\n",
    "        verbose=1,\n",
    "        **PS1_CUT_CONSTANTS,\n",
    "        **tuning_params\n",
    "    )\n",
    "    rate, weight = parse_topline(logs[loader_name])\n",
    "    print(f\"{rate} cutouts/s, {weight} MB / cutout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## simple cutout visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clusters = groupby(get(\"obj_id\"), cuts)\n",
    "clipper = curry(centile_clip)(centiles=(25, 99.9))\n",
    "images = {}\n",
    "for obj_id, cluster in clusters.items():\n",
    "    channels = list(map(compose(clipper, get_in((\"arrays\", 0))), cluster))\n",
    "    channels.append(np.abs(channels[0] - channels[1]))\n",
    "    images[obj_id] = np.dstack(tuple(map(normalize_range, channels)))\n",
    "\n",
    "plt.close('all')\n",
    "for obj_id, image in images.items():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    fig.suptitle(obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32f4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## on-prem ps1 cutout service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9b0bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# comparison to the PS1 cutout service. We can crank up the number of threads we're using...\n",
    "# but at some point we will essentially be attacking the service; no one else will be able to use it\n",
    "# (or it will be forced to block us).\n",
    "# also note that the PS1 cutout service _also_ performs serverside caching, so executing\n",
    "# multiple requests in a row against the same group of images will result in increased performance.\n",
    "\n",
    "REQUEST_THREADS = None\n",
    "watch, netstat = Stopwatch(silent=True), Netstat()\n",
    "watch.start(), netstat.update()\n",
    "req_cutouts = {}\n",
    "request_pool = Pool(REQUEST_THREADS) if REQUEST_THREADS is not None else None\n",
    "\n",
    "for target in targets:\n",
    "    for band in PS1_BANDS:\n",
    "        args = (\n",
    "            ps1_stack_path(target['proj_cell'], target['sky_cell'], band),\n",
    "            target['ra'],\n",
    "            target['dec'],\n",
    "            (CUT_SHAPE[0] + CUT_SHAPE[1]) / 2 * 3600,\n",
    "            \"fits\"\n",
    "        )\n",
    "        if request_pool is None:\n",
    "            req_cutouts[target['obj_id']] = request_ps1_cutout(*args)\n",
    "        else:\n",
    "            req_cutouts[target['obj_id']] = request_pool.apply_async(\n",
    "                request_ps1_cutout, args\n",
    "            )\n",
    "if request_pool is not None:\n",
    "    req_cutouts = {\n",
    "        obj_id: result.get() for obj_id, result in req_cutouts.items()\n",
    "    }\n",
    "netstat.update()\n",
    "count = len(targets) * len(PS1_BANDS)\n",
    "sec = watch.peek()\n",
    "vol = list(netstat.total.values())[-1] / 1024 ** 2\n",
    "print(\n",
    "    f\"made {count} cutouts,{sec} total seconds,{round(vol, 2)} total MB,\\n\"\n",
    "    f\"{round(count / sec, 2)} cutouts / s,{round(vol / count, 2)} MB/cutout\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
