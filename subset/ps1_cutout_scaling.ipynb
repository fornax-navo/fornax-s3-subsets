{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import killscreen.shortcuts as ks\n",
    "from cytoolz.curried import get, get_in, groupby, keyfilter, merge\n",
    "from killscreen import subutils\n",
    "from killscreen.aws import ec2\n",
    "from killscreen.monitors import make_monitors\n",
    "from more_itertools import distribute\n",
    "from pyarrow import parquet\n",
    "\n",
    "# hacky; can remove if we decide to add an install script or put this in the repo root\n",
    "os.chdir(globals()['_dh'][0].parent)\n",
    "\n",
    "from subset.science.handlers import (\n",
    "    filter_ps1_catalog, sample_ps1_catalog, get_corresponding_images\n",
    ")\n",
    "from subset.utilz.generic import parse_topline\n",
    "from subset.utilz.mount_s3 import mount_bucket\n",
    "\n",
    "# suppress irrelevant warnings from matplotlib\n",
    "warnings.filterwarnings(\"ignore\", message=\"More than 20 figures\")\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# username on worker-node instances\n",
    "UNAME = \"ubuntu\"\n",
    "# path on the local filesystem we'll use to collect cutouts from worker nodes\n",
    "DUMP_PATH = '/home/ubuntu/.slice_test/'\n",
    "os.makedirs(DUMP_PATH, exist_ok=True)\n",
    "# where, on the local filesystem, shall we mount that bucket\n",
    "S3_ROOT = '/home/ubuntu/s3'\n",
    "if not os.path.exists(S3_ROOT):\n",
    "    os.mkdir(S3_ROOT)\n",
    "# bucket (meta)data is staged in\n",
    "BUCKET=\"nishapur\"\n",
    "# name of launch template (not included); assumes that Name tag == template name\n",
    "LAUNCH_TEMPLATE = 'fornax-slice'\n",
    "# mount bucket to fetch metadata\n",
    "mount_bucket(backend=\"goofys\", mount_path=S3_ROOT, bucket=BUCKET)\n",
    "# catalog of all mean objects from 1000 PS1 sky cells randomly selected from\n",
    "# \"extragalactic\" cells that overlap the viewports of GALEX visits, then filtered\n",
    "# to the \"best\" objects (qualityFlag bit 0b100000) with valid photometry in both\n",
    "# g and z bands (this filter leaves roughly 3% of total sources). other\n",
    "# similarly-formatted catalog files can be used.\n",
    "CATALOG_FN = \"ps1_eg_eclipses_subset_best_gz_coregistered.parquet\"\n",
    "if not Path(CATALOG_FN).exists():\n",
    "    shutil.copy(Path(S3_ROOT, \"ps1/metadata\", CATALOG_FN), Path(CATALOG_FN))\n",
    "catalog = parquet.read_table(CATALOG_FN).to_pandas()\n",
    "# cutouts in dimensions: ra, dec in degrees. treated as side lengths of a rectangle.\n",
    "CUT_SHAPE = (60 / 3600, 60 / 3600)\n",
    "# restrict to sources bright in both g and z? set to 'None' for no cutoff.\n",
    "MAG_CUTOFF = 20\n",
    "# restrict to only sources flagged as extended / not extended?\n",
    "# \"extended\", \"point\", or None for no restriction\n",
    "EXTENSION_TYPE = \"extended\"\n",
    "# restrict to only sources with a valid stack detection? (probably a good idea)\n",
    "STACK_ONLY = True\n",
    "# how many targets shall we randomly select?\n",
    "TARGET_COUNT = 50\n",
    "# optional parameter -- restrict the total number of PS1 source cells to test the\n",
    "# performance effects of denser sampling. 1000 total cells are available in this test set.\n",
    "# note that the total number of images accessed is number of cells * number of bands.\n",
    "MAX_CELL_COUNT = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize a killscreen Cluster\n",
    "descriptions = ec2.ls_instances(name=LAUNCH_TEMPLATE)\n",
    "# ...either from already-running EC2 instances...\n",
    "if len(descriptions) == 0:\n",
    "    cluster = ec2.Cluster.launch(\n",
    "        count=4,\n",
    "        template=LAUNCH_TEMPLATE,\n",
    "        uname=UNAME,\n",
    "        # 'private' because we'll be talking to them from inside AWS\n",
    "        use_private_ip=True\n",
    "    )\n",
    "# ...or from a new fleet request.\n",
    "else:\n",
    "    cluster = ec2.Cluster.from_descriptions(\n",
    "        descriptions, uname=UNAME, use_private_ip=True\n",
    "    )\n",
    "    cluster.start()\n",
    "    [instance.wait_until_running() for instance in cluster.instances]\n",
    "    cluster.add_public_keys()\n",
    "    print(\"\\n\".join([str(i) for i in cluster.instances]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# freshen these instances in case we've made code changes\n",
    "def git_update(*repo_names):\n",
    "    return ks.chain(\n",
    "        [f\"cd {repo}; git clean -d -fx; git pull & cd ~\" for repo in repo_names], \"and\"\n",
    "    )\n",
    "update = git_update(\"fornax-s3-subsets\", \"killscreen\", \"gphoton_working\")\n",
    "updaters = cluster.command(update, _bg=True)\n",
    "# find script / interpreter on remote instances\n",
    "env = cluster.instances[0].conda_env(\"fornax_section\")\n",
    "python = f\"{env}/bin/python\"\n",
    "endpoint = \"/home/ubuntu/fornax-s3-subsets/subset/ps1_cutout_endpoint.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## target selection\n",
    "the next cell picks a random sample of targets that satisfy the parameters defined above.\n",
    "you can run it again to 'reroll' and pick a new set of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all sources that fit characteristic criteria\n",
    "candidate_sources = filter_ps1_catalog(catalog, MAG_CUTOFF, EXTENSION_TYPE, STACK_ONLY)\n",
    "# randomly-selected subset of those sources w/adequate metadata for cutout definition\n",
    "targets = sample_ps1_catalog(candidate_sources, TARGET_COUNT, MAX_CELL_COUNT)\n",
    "# prune irrelevant fields from targets\n",
    "interesting_fields = ('obj_id', 'proj_cell', 'sky_cell', 'ra', 'dec')\n",
    "targets = [keyfilter(lambda k: k in interesting_fields, t) for t in targets]\n",
    "# add requested cut shape instructions to these target definitions\n",
    "targets = [t | {'ra_x': CUT_SHAPE[0], 'dec_x': CUT_SHAPE[1]} for t in targets]\n",
    "# make lists of the ps1 stack images these sources lie within\n",
    "# (so that we can easily initialize each relevant image only once)\n",
    "ps1_stacks, _ = get_corresponding_images(targets)\n",
    "# split these into chunks of work, making sure that all targets within\n",
    "# a single cell / image are assigned to the same instance --\n",
    "target_groups = groupby(get(['proj_cell', 'sky_cell']), targets)\n",
    "# this is a simple heuristic to distribute work evenly, given the above constraint:\n",
    "groups = sorted(target_groups.values(), key=lambda v: 1 / len(v))\n",
    "work_chunks = [\n",
    "    tuple(chain.from_iterable(chunk))\n",
    "    for chunk in distribute(len(cluster.instances), groups)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple process join function\n",
    "def wait_on(processes, polling_delay=0.1):\n",
    "    while any([p.is_alive() for p in processes]):\n",
    "        time.sleep(polling_delay)\n",
    "\n",
    "# when a remote process is done, grab the files from that instance\n",
    "getters = []\n",
    "def grab_when_done(process, *_):\n",
    "    print(f\"{process.host.ip} done; getting files\")\n",
    "    getter = process.host.get(f\"{DUMP_PATH}*\", DUMP_PATH, _bg=True)\n",
    "    getters.append(getter)\n",
    "        \n",
    "# delete everything local so as to avoid confusion\n",
    "subutils.run(f\"rm {DUMP_PATH}/* &\")\n",
    "\n",
    "# set up some basic benchmarking...\n",
    "stat, note = make_monitors(silent=True)\n",
    "# ...and initiate the remote processes\n",
    "remote_processes = []\n",
    "for chunk, instance in zip(work_chunks, cluster.instances):\n",
    "    command = f\"{python} {endpoint} '{chunk}'\"\n",
    "    viewer = instance.command(\n",
    "        command, _bg=True, _viewer=True, _done=grab_when_done\n",
    "    )\n",
    "    remote_processes.append(viewer)\n",
    "wait_on(remote_processes)\n",
    "note(roundstring(f\"remote processes completed,{stat(simple_cpu=True)}\"), True)\n",
    "wait_on(getters)\n",
    "note(roundstring(f\"cleaned up files from remotes,{stat(simple_cpu=True)}\"), True)\n",
    "\n",
    "retrieved_dumps = os.listdir(DUMP_PATH)\n",
    "\n",
    "cutfiles = tuple(filter(lambda f: f.endswith(\"pkl\"), retrieved_dumps))\n",
    "note(roundstring(f\"got {len(targets) * 2} cuts,{stat(total=True, simple_cpu=True)}\"), True)\n",
    "log = note(None, eject=True)\n",
    "rate, weight = parse_topline(log)\n",
    "print(f\"{rate} cutouts/s, {weight} MB / cutout (local only)\")\n",
    "\n",
    "# cleanup cached arrays on remotes\n",
    "deletions = cluster.command(f\"rm {DUMP_PATH}/* &\", _bg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# should you like: examine logs from remotes...\n",
    "import pandas as pd\n",
    "logs = []\n",
    "for logfile in filter(lambda f: f.endswith(\"csv\"), retrieved_dumps):\n",
    "    remote_log = pd.read_csv(Path(DUMP_PATH, logfile))\n",
    "    remote_log[\"host\"] = re.search(\n",
    "        r\"(?<=ip_)(\\d+_){4}\", logfile\n",
    "    ).group(0)[:-1]\n",
    "    logs.append(remote_log)\n",
    "logs = pd.concat(logs)\n",
    "logs.columns = [\"timestamp\", \"event\", \"duration\", \"volume\", \"cpu\", \"host\"]\n",
    "logs.sort_values(by=[\"host\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ...or your winnings\n",
    "cuts = []\n",
    "for file in cutfiles:\n",
    "    with open(Path(DUMP_PATH, file), \"rb\") as stream:\n",
    "        cuts.append(pickle.load(stream))\n",
    "arrays = tuple(map(get_in(['arrays', 0]), chain.from_iterable(cuts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2afa14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, grid = plt.subplot_mosaic(np.arange(9).reshape(3,3))\n",
    "plt.close()\n",
    "for ax in grid.values():\n",
    "    ax.set_axis_off()\n",
    "\n",
    "for ix in grid.keys():\n",
    "    array = arrays[choice(range(len(arrays)))]\n",
    "    clipped = np.clip(array, *np.percentile(array, (1, 99)))\n",
    "    grid[ix].imshow(clipped, cmap='autumn')\n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97569b7a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# destroy the cluster if you are done with it\n",
    "cluster.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
